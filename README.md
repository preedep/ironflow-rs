# IronFlow-Rs

A high-performance, enterprise-grade Rust-based worker agent designed as a Control-M replacement. IronFlow-Rs works as a Celery-like worker that processes tasks from message queues (Redis, RabbitMQ) with support for command execution, file watching, file transfers, and secret management.

## Features

### Core Capabilities

- **üîÑ Multiple Queue Support**: Redis and RabbitMQ with pluggable architecture for adding more
- **üìÅ File Watcher**: Monitor file system changes with pattern matching and trigger tasks
- **‚ö° Command Execution**: Execute commands with single-shot or polling modes
- **üì¶ File Transfer**: Enterprise-grade file transfers supporting:
  - Local filesystem
  - FTP/FTPS
  - SFTP
  - S3 (AWS, MinIO, etc.)
  - Azure Blob Storage
  - Multi-cloud support via OpenDAL
- **üîê Secret Management**: Integrated support for:
  - HashiCorp Vault
  - AWS Secrets Manager
  - Azure Key Vault
- **üìä Structured Logging**: Based on [standard-app-log](https://github.com/preedep/standard-app-log) specification
- **üèóÔ∏è Clean Architecture**: Domain-driven design with clear separation of concerns
- **‚úÖ Cross-Platform**: Supports Windows, Linux, and macOS

### Task Types

1. **ExecuteCommand**: Run a command and return immediately
2. **ExecutePolling**: Run a command and poll until completion
3. **FileWatch**: Monitor file system changes and trigger actions
4. **FileTransfer**: Transfer files between various storage systems
5. **Composite**: Execute multiple tasks in sequence with error handling

## Architecture

### End-to-End Flow

IronFlow-Rs works as a Celery-compatible worker that integrates with Airflow DAGs and other task orchestrators:

```mermaid
sequenceDiagram
    participant Airflow as Airflow DAG
    participant Queue as Message Queue<br/>(Redis/RabbitMQ)
    participant Worker as IronFlow-Rs<br/>Worker
    participant Secrets as Secret Store<br/>(Vault/AWS/Azure)
    participant Target as Target System<br/>(File/Command/API)
    
    Note over Airflow,Target: Task Submission Flow
    Airflow->>Queue: 1. Push Task to Queue<br/>(execute_command/file_transfer/etc.)
    
    Note over Worker: Worker Poll Loop
    Worker->>Queue: 2. Poll for Tasks<br/>(blocking receive)
    Queue-->>Worker: 3. Return Task
    
    Note over Worker: Task Processing
    Worker->>Worker: 4. Validate Task
    
    alt Task requires secrets
        Worker->>Secrets: 5a. Resolve Secrets<br/>(credentials/keys)
        Secrets-->>Worker: 5b. Return Secret Values
    end
    
    alt Execute Command Task
        Worker->>Target: 6a. Execute Command<br/>(shell/script)
        Target-->>Worker: 6b. Return stdout/stderr/exit_code
    end
    
    alt File Transfer Task
        Worker->>Target: 6c. Transfer File<br/>(SFTP/S3/Azure/Local)
        Target-->>Worker: 6d. Return bytes transferred
    end
    
    alt File Watch Task
        Worker->>Target: 6e. Monitor File System<br/>(watch patterns)
        Target-->>Worker: 6f. Trigger on changes
    end
    
    alt Composite Task
        loop For each sub-task
            Worker->>Worker: 6g. Execute Sub-task
            Note over Worker: Stop on error if configured
        end
    end
    
    Note over Worker: Result Handling
    Worker->>Worker: 7. Build TaskResult<br/>(success/failure/outputs)
    
    alt Callback URL provided
        Worker->>Airflow: 8a. POST Result to Callback<br/>(HTTP webhook)
    end
    
    Worker->>Queue: 8b. Push Result to Result Queue
    Queue-->>Airflow: 9. Airflow polls results
    
    Note over Airflow: Task Complete
    Airflow->>Airflow: 10. Update DAG State<br/>(success/retry/fail)
```

### System Architecture

IronFlow-Rs follows Clean Architecture principles with three main layers:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      Application Layer                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ  ‚îÇ   Worker     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ TaskProcessor   ‚îÇ          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                       Domain Layer                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ  Entities    ‚îÇ  ‚îÇ Repositories ‚îÇ  ‚îÇ   Services   ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  - Task      ‚îÇ  ‚îÇ - Queue      ‚îÇ  ‚îÇ - Command    ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  - Result    ‚îÇ  ‚îÇ - Secret     ‚îÇ  ‚îÇ - Transfer   ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  - FileEvent ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ - Watcher    ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   Infrastructure Layer                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ   Queues     ‚îÇ  ‚îÇ   Secrets    ‚îÇ  ‚îÇ   Services   ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  - Redis     ‚îÇ  ‚îÇ  - Vault     ‚îÇ  ‚îÇ  - Executor  ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  - RabbitMQ  ‚îÇ  ‚îÇ  - AWS       ‚îÇ  ‚îÇ  - Transfer  ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ  - Azure     ‚îÇ  ‚îÇ  - Watcher   ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Integration with Airflow

```mermaid
graph LR
    subgraph "Airflow Orchestration"
        DAG[Airflow DAG]
        Task1[Task 1: Data Prep]
        Task2[Task 2: File Transfer]
        Task3[Task 3: Process Data]
        Task4[Task 4: Upload Results]
        
        DAG --> Task1
        Task1 --> Task2
        Task2 --> Task3
        Task3 --> Task4
    end
    
    subgraph "Message Queue"
        TaskQ[(Task Queue)]
        ResultQ[(Result Queue)]
    end
    
    subgraph "IronFlow-Rs Workers"
        W1[Worker 1]
        W2[Worker 2]
        W3[Worker 3]
    end
    
    subgraph "Execution Targets"
        CMD[Command<br/>Execution]
        FT[File<br/>Transfer]
        FW[File<br/>Watch]
        COMP[Composite<br/>Tasks]
    end
    
    Task2 -->|Push Task| TaskQ
    Task3 -->|Push Task| TaskQ
    Task4 -->|Push Task| TaskQ
    
    TaskQ -->|Poll| W1
    TaskQ -->|Poll| W2
    TaskQ -->|Poll| W3
    
    W1 --> CMD
    W2 --> FT
    W3 --> FW
    W3 --> COMP
    
    W1 -->|Push Result| ResultQ
    W2 -->|Push Result| ResultQ
    W3 -->|Push Result| ResultQ
    
    ResultQ -->|Poll Results| DAG
    
    style DAG fill:#e1f5ff
    style TaskQ fill:#fff4e1
    style ResultQ fill:#fff4e1
    style W1 fill:#e8f5e9
    style W2 fill:#e8f5e9
    style W3 fill:#e8f5e9
```

## Installation

### Prerequisites

- Rust 1.70 or higher
- Redis or RabbitMQ (depending on your queue choice)
- Optional: Vault, AWS credentials, or Azure credentials for secret management

### Build from Source

```bash
git clone https://github.com/yourusername/ironflow-rs.git
cd ironflow-rs
cargo build --release
```

The binary will be available at `target/release/ironflow-rs`

## Configuration

IronFlow-Rs can be configured via environment variables with the `IRONFLOW_` prefix or a configuration file.

### Environment Variables

```bash
# Worker Configuration
IRONFLOW_WORKER_ID=worker-001
IRONFLOW_WORKER__CONCURRENCY=4

# Queue Configuration (Redis)
IRONFLOW_QUEUE__TYPE=redis
IRONFLOW_QUEUE__URL=redis://localhost:6379
IRONFLOW_QUEUE__TASK_QUEUE=ironflow:tasks
IRONFLOW_QUEUE__RESULT_QUEUE=ironflow:results

# Queue Configuration (RabbitMQ)
IRONFLOW_QUEUE__TYPE=rabbitmq
IRONFLOW_QUEUE__URL=amqp://localhost:5672
IRONFLOW_QUEUE__TASK_QUEUE=ironflow.tasks
IRONFLOW_QUEUE__RESULT_QUEUE=ironflow.results
IRONFLOW_QUEUE__PREFETCH_COUNT=10

# Secret Management (Vault)
IRONFLOW_SECRETS__TYPE=hashicorp_vault
IRONFLOW_SECRETS__URL=http://localhost:8200
IRONFLOW_SECRETS__TOKEN=your-vault-token
IRONFLOW_SECRETS__MOUNT_PATH=secret

# Logging
IRONFLOW_LOGGING__LEVEL=info
IRONFLOW_LOGGING__FORMAT=json
IRONFLOW_LOGGING__SERVICE_ID=ironflow-worker
```

### Configuration File Example

Create a `config.toml` file:

```toml
worker_id = "worker-001"

[app]
id = "ironflow-rs"
version = "0.1.0"
geo_location = "us-east-1"

[queue]
type = "redis"
url = "redis://localhost:6379"
task_queue = "ironflow:tasks"
result_queue = "ironflow:results"

[secrets]
type = "hashicorp_vault"
url = "http://localhost:8200"
token = "your-vault-token"
mount_path = "secret"

[logging]
level = "info"
format = "json"
service_id = "ironflow-worker"
service_version = "0.1.0"

[worker]
concurrency = 4
receive_timeout_secs = 30
max_task_timeout_secs = 3600
shutdown_timeout_secs = 30
health_check_interval_secs = 60
```

## Usage

### Starting the Worker

```bash
# Using environment variables
export IRONFLOW_QUEUE__URL=redis://localhost:6379
./target/release/ironflow-rs

# Or with a config file
IRONFLOW_CONFIG_FILE=config.toml ./target/release/ironflow-rs
```

### Task Examples

#### Execute Command Task

```json
{
  "id": "550e8400-e29b-41d4-a716-446655440000",
  "task_type": {
    "type": "execute_command",
    "command": "python",
    "args": ["script.py", "--input", "data.csv"],
    "working_dir": "/opt/scripts",
    "env_vars": {
      "PYTHONPATH": "/opt/lib"
    }
  },
  "priority": 0,
  "max_retries": 3,
  "callback_url": "https://api.example.com/task-callback",
  "correlation_id": "batch-job-123"
}
```

#### File Transfer Task

```json
{
  "id": "550e8400-e29b-41d4-a716-446655440001",
  "task_type": {
    "type": "file_transfer",
    "source": {
      "type": "sftp",
      "host": "sftp.example.com",
      "port": 22,
      "path": "/data/input.csv",
      "username": "user",
      "key_secret": "vault:ssh/private_key"
    },
    "destination": {
      "type": "s3",
      "bucket": "my-bucket",
      "key": "data/input.csv",
      "region": "us-east-1"
    },
    "options": {
      "overwrite": true,
      "create_dirs": true,
      "preserve_metadata": true
    }
  }
}
```

#### File Watch Task

```json
{
  "id": "550e8400-e29b-41d4-a716-446655440002",
  "task_type": {
    "type": "file_watch",
    "path": "/data/incoming",
    "patterns": ["*.csv", "*.json"],
    "recursive": true,
    "on_change_tasks": [
      {
        "type": "execute_command",
        "command": "process-file",
        "args": ["${file_path}"]
      }
    ]
  }
}
```

#### Composite Task

```json
{
  "id": "550e8400-e29b-41d4-a716-446655440003",
  "task_type": {
    "type": "composite",
    "tasks": [
      {
        "type": "file_transfer",
        "source": {"type": "sftp", "...": "..."},
        "destination": {"type": "local", "path": "/tmp/data.csv"}
      },
      {
        "type": "execute_command",
        "command": "process-data",
        "args": ["/tmp/data.csv"]
      },
      {
        "type": "file_transfer",
        "source": {"type": "local", "path": "/tmp/result.csv"},
        "destination": {"type": "s3", "...": "..."}
      }
    ],
    "stop_on_error": true
  }
}
```

## Sending Tasks to the Queue

### Using Redis

```python
import redis
import json

r = redis.Redis(host='localhost', port=6379)

task = {
    "id": "550e8400-e29b-41d4-a716-446655440000",
    "task_type": {
        "type": "execute_command",
        "command": "echo",
        "args": ["Hello, IronFlow!"]
    },
    "priority": 0,
    "max_retries": 3
}

r.lpush('ironflow:tasks', json.dumps(task))
```

### Using RabbitMQ

```python
import pika
import json

connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))
channel = connection.channel()

channel.queue_declare(queue='ironflow.tasks', durable=True)

task = {
    "id": "550e8400-e29b-41d4-a716-446655440000",
    "task_type": {
        "type": "execute_command",
        "command": "echo",
        "args": ["Hello, IronFlow!"]
    }
}

channel.basic_publish(
    exchange='',
    routing_key='ironflow.tasks',
    body=json.dumps(task),
    properties=pika.BasicProperties(delivery_mode=2)
)

connection.close()
```

## Logging

IronFlow-Rs uses structured logging based on the [standard-app-log](https://github.com/preedep/standard-app-log) specification. All logs are output in JSON format:

```json
{
  "event_date_time": "2024-12-24T06:20:00.000Z",
  "log_type": "APP_LOG",
  "app_id": "ironflow-rs",
  "app_version": "0.1.0",
  "service_id": "ironflow-worker",
  "service_version": "0.1.0",
  "correlation_id": "batch-job-123",
  "level": "info",
  "execution_time": 1250,
  "message": "Task completed successfully"
}
```

## Development

### Running Tests

```bash
# Run all tests
cargo test

# Run tests with output
cargo test -- --nocapture

# Run specific test
cargo test test_execute_command

# Run integration tests (requires Redis/RabbitMQ)
cargo test --test integration -- --ignored
```

### Project Structure

```
ironflow-rs/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ domain/              # Domain layer (business logic)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ entities/        # Domain entities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repositories/    # Repository traits
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/        # Service traits
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ value_objects/   # Value objects and configs
‚îÇ   ‚îú‚îÄ‚îÄ infrastructure/      # Infrastructure layer (implementations)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logging/         # Structured logging
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ queues/          # Queue implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ secrets/         # Secret management
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ services/        # Service implementations
‚îÇ   ‚îú‚îÄ‚îÄ application/         # Application layer (orchestration)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ task_processor.rs
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ worker.rs
‚îÇ   ‚îú‚îÄ‚îÄ lib.rs
‚îÇ   ‚îî‚îÄ‚îÄ main.rs
‚îú‚îÄ‚îÄ Cargo.toml
‚îî‚îÄ‚îÄ README.md
```

## Deployment

### Docker

Create a `Dockerfile`:

```dockerfile
FROM rust:1.70 as builder
WORKDIR /app
COPY . .
RUN cargo build --release

FROM debian:bookworm-slim
RUN apt-get update && apt-get install -y ca-certificates && rm -rf /var/lib/apt/lists/*
COPY --from=builder /app/target/release/ironflow-rs /usr/local/bin/
CMD ["ironflow-rs"]
```

Build and run:

```bash
docker build -t ironflow-rs .
docker run -e IRONFLOW_QUEUE__URL=redis://redis:6379 ironflow-rs
```

### Kubernetes

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ironflow-worker
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ironflow-worker
  template:
    metadata:
      labels:
        app: ironflow-worker
    spec:
      containers:
      - name: worker
        image: ironflow-rs:latest
        env:
        - name: IRONFLOW_QUEUE__URL
          value: "redis://redis-service:6379"
        - name: IRONFLOW_WORKER__CONCURRENCY
          value: "4"
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
```

## Performance

IronFlow-Rs is designed for high performance and low resource usage:

- **Concurrency**: Process multiple tasks simultaneously with configurable worker pools
- **Memory Efficient**: Rust's zero-cost abstractions and ownership model
- **Fast Startup**: Typically starts in under 1 second
- **Low Overhead**: Minimal CPU usage when idle

## Security

- **Secret Management**: Never store secrets in code or configuration files
- **TLS Support**: All network communications can use TLS/SSL
- **Least Privilege**: Run with minimal required permissions
- **Audit Logging**: All operations are logged with correlation IDs

## Contributing

Contributions are welcome! Please follow these guidelines:

1. Fork the repository
2. Create a feature branch
3. Write tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Roadmap

- [ ] Web UI for monitoring and management
- [ ] Metrics and monitoring (Prometheus/Grafana)
- [ ] Additional queue backends (Kafka, NATS)
- [ ] Plugin system for custom task types
- [ ] Distributed tracing support (OpenTelemetry)
- [ ] Task scheduling and cron support
- [ ] Task dependency management

## Support

For issues, questions, or contributions, please open an issue on GitHub.

## Acknowledgments

- Built with [Rust](https://www.rust-lang.org/)
- Uses [OpenDAL](https://opendal.apache.org) for multi-cloud storage
- Logging based on [standard-app-log](https://github.com/preedep/standard-app-log)
- Inspired by Celery and Control-M
